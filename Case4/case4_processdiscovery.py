# -*- coding: utf-8 -*-
"""Case4.ProcessDiscovery.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RtrWqwwpc7scxX0W8B34j1cmyb3kMsPR
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# %pip install pm4py
import pm4py
from pm4py.objects.log.util import dataframe_utils
from pm4py.objects.conversion.log import converter as log_converter

import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

log_csv = pd.read_csv('https://raw.githubusercontent.com/paoloceravolo/PM-Regensburg/main/ArtificialPatientTreatment.csv', sep=',')
log_csv = dataframe_utils.convert_timestamp_columns_in_df(log_csv)

# Create a pivot table of the start (minimum) and end (maximum) timestamps associated with each case:
case_starts_ends = log_csv.pivot_table(index='patient', aggfunc={'datetime': ['min', 'max']})
case_starts_ends = case_starts_ends.reset_index() 
case_starts_ends.columns = ['patient', 'caseend', 'casestart'] 
# Merge with the main event log data so that for each row we have the start and end times.
log_csv = log_csv.merge(case_starts_ends, on='patient')
# Calculate the relative time by subtracting the process start time from the event timestamp
log_csv['cumulativetime'] = log_csv['datetime'] - log_csv['casestart']
# Convert relative times to more friendly measures
## seconds
log_csv['cumulativetime_s'] = log_csv['cumulativetime'].dt.seconds + 86400*log_csv['cumulativetime'].dt.days 
## days
log_csv['cumulativedays'] = log_csv['cumulativetime'].dt.days

print(log_csv)

## Get an array of patient labels for the y axis - for graph labelling purposes
patientnums = [int(e) for e in log_csv['patient'].apply(lambda x: x.strip('patient'))]
## Plot a scatter plot of patient events over relative time
ax = sns.scatterplot(x=log_csv['cumulativetime_s'],
y=log_csv['patient'], hue=log_csv['action'])
## Set y axis ticks so that you only show every 5th patient - for readability
plt.yticks(np.arange(min(patientnums), max(patientnums)+1, 5))

## Create a table giving the number of cases in which each event is present.
patient_events = pd.crosstab(log_csv['patient'], log_csv['action'])
## Visualise in a heatmap
sns.heatmap(patient_events, cmap="YlGnBu")

# Comply to the naming standard of PM4PY

log_csv.rename(columns={'datetime': 'time:timestamp', 
'patient': 'case:concept:name', 'action': 'concept:name', 'resource': 'org:resource'}, inplace=True)

## Convert to log format 
log = log_converter.apply(log_csv)

## Import the dfg_discovery algorithm
from pm4py.algo.discovery.dfg import algorithm as dfg_discovery
## Import the dfg visualization object
from pm4py.visualization.dfg import visualizer as dfg_visualization
#Create graph from log
dfg = dfg_discovery.apply(log)
# Visualise
gviz = dfg_visualization.apply(dfg, log=log, variant=dfg_visualization.Variants.FREQUENCY)
dfg_visualization.view(gviz)

## Alpha_miner algorithm
net, im, fm = pm4py.discover_petri_net_alpha(log_csv)

# Visualise 
pm4py.view_petri_net(net, im, fm, format='png')

# Discover process tree using inductive miner
process_tree = pm4py.discover_process_tree_inductive(log_csv)

pm4py.view_process_tree(process_tree, format='png')

# Discover petri net using inductive miner
net, im, fm = pm4py.discover_petri_net_inductive(log_csv)

pm4py.view_petri_net(net, im, fm, format='png')

footprints = pm4py.discover_footprints(log_csv)
pm4py.view_footprints(footprints, format='png')

prefix_tree = pm4py.discover_prefix_tree(log_csv)
pm4py.view_prefix_tree(prefix_tree, format='png')

tbr_diagnostics = pm4py.conformance_diagnostics_token_based_replay(log_csv, net, im, fm)
diagnostics_df = pd.DataFrame.from_dict(tbr_diagnostics)

diagnostics_df