# -*- coding: utf-8 -*-
"""Case3DecisionTree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1__vfTiDqcFDbEnuCxIsusxBloNy4zpiz
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pm4py
import pandas as pd
import pm4py

# Preparing data

## Let's import the event log dataset in a pandas dataframe
### pm4py uses dataframe as the standard format for event logs

log_df = pd.read_csv('https://raw.githubusercontent.com/paoloceravolo/BIS2022/main/Event%20Logs/Road_Traffic_Fine_Management_Process.csv',sep=',')
log_df.rename(columns={'Case ID': 'case:concept:name', 'Complete Timestamp': 'time:timestamp', 'Activity': 'concept:name', 'Resource': 'org:resource'}, inplace=True) #change the name to a colum
# log_df = pm4py.format_dataframe(log_df, case_id='Case ID', activity_key='Activity', timestamp_key='Complete Timestamp') # DeprecatedWarning: format_dataframe is deprecated as of 2.3.0 and will be removed in 3.0.0.
# event_log = pm4py.convert_to_event_log(log_df)
# print(event_log)
# convert the 'Date' column to datetime format
log_df['time:timestamp']= pd.to_datetime(log_df['time:timestamp'])

num_events = len(log_df)
num_cases = len(log_df['case:concept:name'].unique())
print("Number of events: {}\nNumber of cases: {}".format(num_events, num_cases))

start_activities = pm4py.get_start_activities(log_df)
end_activities = pm4py.get_end_activities(log_df)
print("Start activities: {}\nEnd activities: {}".format(start_activities, end_activities))

#log_df

## Let's filter on cases with duration 0 and not ending with a Payment activity

filtered_log = pm4py.filter_variants(log_df, [('Create Fine', 'Send Fine')], retain=True)
filtered_log = pm4py.filter_case_performance(filtered_log, 0, 0)
variants = pm4py.get_variants(filtered_log)
print(variants)

## Let's remove this log segment from the total event log 
### we use pandas functions to get the difference between data frames
### pm4py unfortunately offers very limited options for set operations with event logs 

#log_df_diff = log_df.compare(filtered_log) # working only if the dataframe have the same index
log_df = pd.concat([log_df,filtered_log]).drop_duplicates(keep=False)

num_events = len(log_df)
num_cases = len(log_df['case:concept:name'].unique())
print("Number of events: {}\nNumber of cases: {}".format(num_events, num_cases))

# Incomplete cases

## Let's consider incomplete all cases not ending with a legal end activity like Payment or Send for Credit Collection

filtered_log = pm4py.filter_end_activities(log_df, ['Payment', 'Send for Credit Collection', 'Send Appeal to Prefecture', 'Appeal to Judge'])

## Let's filter traces by attribute value
### we know 'Send Appeal to Prefecture' and 'Appeal to Judge' can be considered a legal end activity only if the value of the 'dismissal' attribute is equal to '#' or 'G' respectively 

filtered_log_att = pm4py.filter_end_activities(log_df, ['Send Appeal to Prefecture', 'Appeal to Judge'])
filtered_log_att = pm4py.filter_trace_attribute_values(filtered_log_att, 'dismissal', ['#', 'G'])

print("Given {} total cases in the log we have {} cases that comply with the applied filter".format(len(filtered_log['case:concept:name'].unique()), len(filtered_log_att['case:concept:name'].unique())))

## Let's merge the log segments we obtained 

filtered_log = pd.concat([filtered_log,filtered_log_att])

num_events = len(filtered_log)
num_cases = len(filtered_log['case:concept:name'].unique())
print("Number of events: {}\nNumber of cases: {}".format(num_events, num_cases))

## Let's extract the top k variants

filtered_log = pm4py.filter_variants_top_k(filtered_log, 10)

num_events = len(filtered_log)
num_cases = len(filtered_log['case:concept:name'].unique())
print("Number of events: {}\nNumber of cases: {}".format(num_events, num_cases))

filtered_log

## Trace Encoding - Feature Extraction 

#Repalce NaN with zero on all columns 
filtered_log = filtered_log.fillna(0)
filtered_log = filtered_log.replace('NIL', 0)
# filtered_log = filtered_log.drop(['case:concept:name'], axis='columns') #Drop non numerical columns

#Extract the features and encode them into onehot encoding vector
features_df = pm4py.extract_features_dataframe(filtered_log, str_ev_attr = ['concept:name', 'amount', 'paymentAmount'])

#Normalize the columns
from sklearn.preprocessing import MaxAbsScaler # Maximum absolute scaling rescales each feature by the maximum absolute value
from scipy import stats

norm_df = features_df.drop(['case:concept:name'], axis='columns') #Drop non numerical columns
column_names = norm_df.columns.values.tolist() #get the list of all column names from headers
transformer = MaxAbsScaler().fit(norm_df) # Maximum absolute scaling rescales each feature by the maximum absolute value
norm_df = pd.DataFrame(transformer.transform(norm_df))
#norm_df = pd.DataFrame(stats.zscore(norm_df)) # Zero Score (or z-score) transforms the features of the column into values with a mean of 0 and a standard deviation of 1
norm_df.columns = column_names # adding column name to the respective columns

norm_df

# Load libraries
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation

#split dataset in features and target variable
#print(column_names)
feature_cols =  [x for x in column_names if x != "concept:name_Payment"]
feature_cols =  [x for x in feature_cols if x != "paymentAmount"]
#print(feature_cols)

X = norm_df[feature_cols] # Features, can be numerical or categorical
y = norm_df['concept:name_Payment'] # Target variable, must be categorical

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test#

# Create Decision Tree classifer object
clf = DecisionTreeClassifier()

# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)

#Predict the response for test dataset
y_pred = clf.predict(X_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

from six import StringIO 
from IPython.display import Image  
from sklearn.tree import export_graphviz
import pydotplus
dot_data = StringIO()
export_graphviz(clf, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True, feature_names = feature_cols,class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('diabetes.png')
Image(graph.create_png())

#Extracts a dataframe containing the temporal features of the provided log object
temporal_features_df = pm4py.extract_temporal_features_dataframe(filtered_log, grouper_freq = 'M')

temporal_features_df

# Extracts from a log object the target vector for a specific ML use case (next activity, next time, remaining time)
vector_next_act, class_next_act = pm4py.extract_target_vector(filtered_log, 'next_activity')
vector_next_time, class_next_time = pm4py.extract_target_vector(filtered_log, 'next_time')
vector_rem_time, class_rem_time = pm4py.extract_target_vector(filtered_log, 'remaining_time')

print(vector_next_act)