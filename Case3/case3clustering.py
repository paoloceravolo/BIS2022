# -*- coding: utf-8 -*-
"""Case3Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hRzL3-1gD6ip5xFr6jsgMN6o1VOdAom1
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pm4py
import pandas as pd
import pm4py

# Preparing data

## Let's import the event log dataset in a pandas dataframe
### pm4py uses dataframe as the standard format for event logs

log_df = pd.read_csv('https://raw.githubusercontent.com/paoloceravolo/BIS2022/main/Event%20Logs/Road_Traffic_Fine_Management_Process.csv',sep=',')
log_df.rename(columns={'Case ID': 'case:concept:name', 'Complete Timestamp': 'time:timestamp', 'Activity': 'concept:name', 'Resource': 'org:resource'}, inplace=True) #change the name to a colum
# log_df = pm4py.format_dataframe(log_df, case_id='Case ID', activity_key='Activity', timestamp_key='Complete Timestamp') # DeprecatedWarning: format_dataframe is deprecated as of 2.3.0 and will be removed in 3.0.0.
# event_log = pm4py.convert_to_event_log(log_df)
# print(event_log)
# convert the 'Date' column to datetime format
log_df['time:timestamp']= pd.to_datetime(log_df['time:timestamp'])

num_events = len(log_df)
num_cases = len(log_df['case:concept:name'].unique())
print("Number of events: {}\nNumber of cases: {}".format(num_events, num_cases))

start_activities = pm4py.get_start_activities(log_df)
end_activities = pm4py.get_end_activities(log_df)
print("Start activities: {}\nEnd activities: {}".format(start_activities, end_activities))

#log_df

## Let's filter on cases with duration 0 and not ending with a Payment activity

filtered_log = pm4py.filter_variants(log_df, [('Create Fine', 'Send Fine')], retain=True)
filtered_log = pm4py.filter_case_performance(filtered_log, 0, 0)
variants = pm4py.get_variants(filtered_log)
print(variants)

## Let's remove this log segment from the total event log 
### we use pandas functions to get the difference between data frames
### pm4py unfortunately offers very limited options for set operations with event logs 

#log_df_diff = log_df.compare(filtered_log) # working only if the dataframe have the same index
log_df = pd.concat([log_df,filtered_log]).drop_duplicates(keep=False)

num_events = len(log_df)
num_cases = len(log_df['case:concept:name'].unique())
print("Number of events: {}\nNumber of cases: {}".format(num_events, num_cases))

# Incomplete cases

## Let's consider incomplete all cases not ending with a legal end activity like Payment or Send for Credit Collection

filtered_log = pm4py.filter_end_activities(log_df, ['Payment', 'Send for Credit Collection', 'Send Appeal to Prefecture', 'Appeal to Judge'])

## Let's filter traces by attribute value
### we know 'Send Appeal to Prefecture' and 'Appeal to Judge' can be considered a legal end activity only if the value of the 'dismissal' attribute is equal to '#' or 'G' respectively 

filtered_log_att = pm4py.filter_end_activities(log_df, ['Send Appeal to Prefecture', 'Appeal to Judge'])
filtered_log_att = pm4py.filter_trace_attribute_values(filtered_log_att, 'dismissal', ['#', 'G'])

print("Given {} total cases in the log we have {} cases that comply with the applied filter".format(len(filtered_log['case:concept:name'].unique()), len(filtered_log_att['case:concept:name'].unique())))

## Let's merge the log segments we obtained 

filtered_log = pd.concat([filtered_log,filtered_log_att])

num_events = len(filtered_log)
num_cases = len(filtered_log['case:concept:name'].unique())
print("Number of events: {}\nNumber of cases: {}".format(num_events, num_cases))

## Let's extract the top k variants

filtered_log = pm4py.filter_variants_top_k(filtered_log, 10)

num_events = len(filtered_log)
num_cases = len(filtered_log['case:concept:name'].unique())
print("Number of events: {}\nNumber of cases: {}".format(num_events, num_cases))

## Trace Encoding - Feature Extraction 

#Extract the features and encode them into onehot encoding vector
features_df = pm4py.extract_features_dataframe(filtered_log, str_ev_attr = ['concept:name', 'amount', 'paymentAmount'])

#Repalce NaN with zero on all columns 
features_df = features_df.fillna(0)


#Normalize the values
from sklearn import preprocessing

norm_df = features_df.drop(['case:concept:name'], axis='columns') #Drop non numerical columns
column_names = norm_df.columns.values.tolist() #get the list of all column names from headers
x = norm_df.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler() #normilising with MinMaxScaler #be careful becase when a value is always 1 the min max produce 0
x_scaled = min_max_scaler.fit_transform(x)
norm_df = pd.DataFrame(x_scaled)
norm_df.columns = column_names # adding column name to the respective columns


norm_df

# K-MEANS CLUSTERING
# Importing Modules
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

#initialize kmeans parameters
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"random_state": None,
}

#create list to hold SSE (sum of squared errors) values for each k
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(norm_df)
    sse.append(kmeans.inertia_)

#visualize results
plt.plot(range(1, 11), sse)
plt.xticks(range(1, 11))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()

#instantiate the k-means class, using optimal number of clusters
kmeans = KMeans(init="random", n_clusters=3, n_init=10, random_state=1)

#fit k-means algorithm to data
kmeans.fit(norm_df)

#view cluster assignments for each observation
kmeans.labels_

#append cluster assingments to original DataFrame
features_df['cluster'] = kmeans.labels_

#view updated DataFrame
features_df

# View event log by cluster

# get the list of cases having 0 as a value of column cluster 
cluster1_cases = features_df[features_df['cluster'] == 0]['case:concept:name'].tolist()
#print(len(cluster1_cases))
# selecting rows based on condition 
cluster1_log = filtered_log[filtered_log['case:concept:name'].isin(cluster1_cases)]

num_events = len(cluster1_log)
num_cases = len(cluster1_log['case:concept:name'].unique())
print("Number of events: {}\nNumber of cases: {}".format(num_events, num_cases))

# get the list of cases having 1 as a value of column cluster 
cluster2_cases = features_df[features_df['cluster'] == 1]['case:concept:name'].tolist()
# selecting rows based on condition 
cluster2_log = filtered_log[filtered_log['case:concept:name'].isin(cluster2_cases)]

num_events = len(cluster2_log)
num_cases = len(cluster2_log['case:concept:name'].unique())
print("Number of events: {}\nNumber of cases: {}".format(num_events, num_cases))

# get the list of cases having 2 as a value of column cluster 
cluster3_cases = features_df[features_df['cluster'] == 2]['case:concept:name'].tolist()
# selecting rows based on condition 
cluster3_log = filtered_log[filtered_log['case:concept:name'].isin(cluster3_cases)]

num_events = len(cluster3_log)
num_cases = len(cluster3_log['case:concept:name'].unique())
print("Number of events: {}\nNumber of cases: {}".format(num_events, num_cases))

## Convert to csv and download file  

from google.colab import files

features_df.to_csv('output.csv', encoding = 'utf-8-sig') 
files.download('output.csv')

# create and visualize the petri net on the filtered_log using inductive miner
net, im, fm = pm4py.discover_petri_net_inductive(filtered_log)
pm4py.view_petri_net(net, im, fm, format='png')

# create and visualize the petri net on clsuter1 using inductive miner
net, im, fm = pm4py.discover_petri_net_inductive(cluster1_log)
pm4py.view_petri_net(net, im, fm, format='png')

# create and visualize the petri net on clsuter2 using inductive miner
net, im, fm = pm4py.discover_petri_net_inductive(cluster2_log)
pm4py.view_petri_net(net, im, fm, format='png')

# create and visualize the petri net on clsuter3 using inductive miner
net, im, fm = pm4py.discover_petri_net_inductive(cluster3_log)
pm4py.view_petri_net(net, im, fm, format='png')

# Using OpenAI generate an abstraction of the event log for stuiding the statistical distribution of the data

print(pm4py.openai.abstract_log_attributes(features_df))